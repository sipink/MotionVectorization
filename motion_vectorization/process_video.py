"""
Motion Vectorization Video Processor - Main Entry Point
Coordinates SAM2.1, CoTracker3, and FlowSeek for maximum accuracy motion graphics

This module provides the single entry point for processing videos with all AI engines.
Handles the complete pipeline from video input to motion data output.
"""

import os
import sys
import json
import pickle
import time
import argparse
import numpy as np
import cv2
import torch
import warnings
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass, asdict
import shutil

# Import engine status checker
from .engine_status import verify_engines_at_startup, get_active_engines

# Import core engines with graceful fallback
ENGINE_MODULES = {}

# Try importing SAM2 engine
try:
    from .sam2_engine import SAM2SegmentationEngine, SAM2Config
    ENGINE_MODULES['sam2'] = True
    print("âœ… SAM2 engine module imported")
except ImportError as e:
    ENGINE_MODULES['sam2'] = False
    print(f"âš ï¸ SAM2 engine not available: {e}")
    
# Try importing CoTracker3 engine  
try:
    from .cotracker3_engine import CoTracker3TrackerEngine, CoTracker3Config
    ENGINE_MODULES['cotracker3'] = True
    print("âœ… CoTracker3 engine module imported")
except ImportError as e:
    ENGINE_MODULES['cotracker3'] = False
    print(f"âš ï¸ CoTracker3 engine not available: {e}")

# Try importing FlowSeek engine
try:
    from .flowseek_engine import FlowSeekEngine, FlowSeekConfig, MotionBasisDecomposer
    ENGINE_MODULES['flowseek'] = True
    print("âœ… FlowSeek engine module imported")
except ImportError as e:
    ENGINE_MODULES['flowseek'] = False
    print(f"âš ï¸ FlowSeek engine not available: {e}")

# Import dataloader and utilities
from .dataloader import DataLoader
from .utils import (
    compute_clusters_floodfill, clean_labels, is_valid_cluster,
    get_shape_coords, get_shape_centroid, get_shape_mask,
    save_frames, get_cmap
)


@dataclass
class VideoProcessorConfig:
    """Configuration for video processing pipeline"""
    video_file: str
    video_dir: str = 'videos'
    output_dir: str = 'motion_vectorization/outputs'
    suffix: Optional[str] = None
    
    # Processing settings
    max_frames: int = 200
    start_frame: int = 1
    base_frame: int = 0
    
    # AI engine settings
    use_sam2: bool = True
    use_cotracker3: bool = True
    use_flowseek: bool = True
    
    # Quality settings
    quality_threshold: float = 0.95
    min_cluster_size: int = 50
    min_density: float = 0.15
    
    # Device settings
    device: str = "auto"
    
    def __post_init__(self):
        """Auto-configure device"""
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"


class MotionVectorizationProcessor:
    """
    Main processor that coordinates all AI engines for motion vectorization
    """
    
    def __init__(self, config: VideoProcessorConfig):
        self.config = config
        self.device = torch.device(config.device)
        
        # Initialize engine status
        print("\nðŸ” Checking engine availability...")
        self.engine_status = verify_engines_at_startup(verbose=False)
        self.active_engines = get_active_engines(self.engine_status)
        
        # Initialize AI engines
        self.sam2_engine = None
        self.cotracker3_engine = None
        self.flowseek_engine = None
        
        self._initialize_engines()
        
        # Setup paths
        self.video_name = Path(config.video_file).stem
        self.video_folder = Path(config.video_dir) / self.video_name
        self.rgb_folder = self.video_folder / 'rgb'
        self.flow_folder = self.video_folder / 'flow'
        self.output_folder = Path(config.output_dir) / f"{self.video_name}_{config.suffix}"
        
    def _initialize_engines(self):
        """Initialize available AI engines"""
        
        # Initialize SAM2 if available
        if self.config.use_sam2 and ENGINE_MODULES.get('sam2') and 'sam2' in self.active_engines:
            try:
                print("ðŸŽ¯ Initializing SAM2.1 segmentation engine...")
                try:
                    from .sam2_engine import SAM2Config, SAM2SegmentationEngine
                    sam2_config = SAM2Config(
                        device=str(self.device),
                        accuracy_threshold=self.config.quality_threshold
                    )
                    self.sam2_engine = SAM2SegmentationEngine(sam2_config)
                except ImportError:
                    print("âŒ SAM2 engine not available")
                    self.sam2_engine = None
                print("âœ… SAM2.1 engine ready")
            except Exception as e:
                print(f"âŒ SAM2.1 initialization failed: {e}")
                self.sam2_engine = None
        
        # Initialize CoTracker3 if available
        if self.config.use_cotracker3 and ENGINE_MODULES.get('cotracker3') and 'cotracker3' in self.active_engines:
            try:
                print("ðŸŽ¬ Initializing CoTracker3 tracking engine...")
                try:
                    from .cotracker3_engine import CoTracker3Config, CoTracker3TrackerEngine
                    cotracker3_config = CoTracker3Config(
                        device=str(self.device),
                        model_variant="cotracker3_offline"
                    )
                    self.cotracker3_engine = CoTracker3TrackerEngine(cotracker3_config)
                except ImportError:
                    print("âŒ CoTracker3 engine not available")
                    self.cotracker3_engine = None
                print("âœ… CoTracker3 engine ready")
            except Exception as e:
                print(f"âŒ CoTracker3 initialization failed: {e}")
                self.cotracker3_engine = None
        
        # Initialize FlowSeek if available
        if self.config.use_flowseek and ENGINE_MODULES.get('flowseek') and 'flowseek' in self.active_engines:
            try:
                print("ðŸŒŠ Initializing FlowSeek optical flow engine...")
                try:
                    from .flowseek_engine import FlowSeekConfig, FlowSeekEngine
                    flowseek_config = FlowSeekConfig(
                        device=str(self.device)
                        # Note: removed accuracy_mode as it's not a valid parameter
                    )
                    self.flowseek_engine = FlowSeekEngine(flowseek_config)
                except ImportError:
                    print("âŒ FlowSeek engine not available")
                    self.flowseek_engine = None
                print("âœ… FlowSeek engine ready")
            except Exception as e:
                print(f"âŒ FlowSeek initialization failed: {e}")
                self.flowseek_engine = None
    
    def process_video(self) -> Dict[str, Any]:
        """
        Main processing pipeline - coordinates all AI engines
        """
        print(f"\nðŸš€ Processing video: {self.config.video_file}")
        print(f"ðŸ“Š Active engines: {', '.join(self.active_engines) if self.active_engines else 'None (fallback mode)'}")
        
        # Create output directories
        self._create_directories()
        
        # Load video frames
        frames = self._load_frames()
        if len(frames) == 0:
            raise ValueError(f"No frames found in {self.rgb_folder}")
        
        print(f"ðŸ“¸ Loaded {len(frames)} frames")
        
        # Process with available engines
        results = {
            'frames': frames,
            'segmentation': None,
            'tracking': None,
            'optical_flow': None,
            'shapes': None
        }
        
        # 1. SAM2 Segmentation
        if self.sam2_engine is not None:
            print("\nðŸŽ¯ Running SAM2.1 segmentation...")
            results['segmentation'] = self._run_sam2_segmentation(frames)
        else:
            print("\nâš ï¸ SAM2 not available - using fallback segmentation")
            results['segmentation'] = self._fallback_segmentation(frames)
        
        # 2. FlowSeek Optical Flow
        if self.flowseek_engine is not None:
            print("\nðŸŒŠ Running FlowSeek optical flow...")
            results['optical_flow'] = self._run_flowseek(frames)
        else:
            print("\nâš ï¸ FlowSeek not available - using fallback optical flow")
            results['optical_flow'] = self._fallback_optical_flow(frames)
        
        # 3. CoTracker3 Point Tracking
        if self.cotracker3_engine is not None:
            print("\nðŸŽ¬ Running CoTracker3 point tracking...")
            results['tracking'] = self._run_cotracker3(frames, results['segmentation'])
        else:
            print("\nâš ï¸ CoTracker3 not available - using fallback tracking")
            results['tracking'] = self._fallback_tracking(frames, results['segmentation'])
        
        # 4. Extract and optimize shapes
        print("\nðŸ”· Extracting and optimizing shapes...")
        results['shapes'] = self._extract_shapes(results)
        
        # 5. Save outputs
        print("\nðŸ’¾ Saving outputs...")
        self._save_outputs(results)
        
        print(f"\nâœ… Processing complete! Outputs saved to {self.output_folder}")
        
        return results
    
    def _create_directories(self):
        """Create necessary output directories"""
        self.output_folder.mkdir(parents=True, exist_ok=True)
        (self.output_folder / 'debug').mkdir(exist_ok=True)
        (self.output_folder / 'outputs').mkdir(exist_ok=True)
        (self.output_folder / 'shapes').mkdir(exist_ok=True)
        
        # Create flow directories if needed
        if not self.flow_folder.exists():
            self.flow_folder.mkdir(parents=True, exist_ok=True)
            (self.flow_folder / 'forward').mkdir(exist_ok=True)
            (self.flow_folder / 'backward').mkdir(exist_ok=True)
            (self.flow_folder / 'viz').mkdir(exist_ok=True)
    
    def _load_frames(self) -> np.ndarray:
        """Load RGB frames from disk"""
        frames = []
        frame_files = sorted(self.rgb_folder.glob('*.png'))
        
        for frame_file in frame_files[:self.config.max_frames]:
            frame = cv2.imread(str(frame_file))
            if frame is not None:
                frames.append(frame)
        
        return np.array(frames) if frames else np.array([])
    
    def _run_sam2_segmentation(self, frames: np.ndarray) -> Dict[str, Any]:
        """Run SAM2.1 segmentation on frames"""
        try:
            # Process frames with SAM2
            # Use available method on SAM2 engine
            if hasattr(self.sam2_engine, 'segment_frames'):
                segmentation_results = self.sam2_engine.segment_frames(frames)  # type: ignore
            elif hasattr(self.sam2_engine, 'process'):
                segmentation_results = self.sam2_engine.process(frames)  # type: ignore
            else:
                segmentation_results = {'masks': [], 'quality_score': 0.0}
            
            # Generate labels, fgbg, and comps
            labels = []
            fgbg = []
            comps = []
            
            for frame_idx, masks in enumerate(segmentation_results.get('masks', [])):
                # Create label map from masks
                label_map = np.zeros(frames[frame_idx].shape[:2], dtype=np.int32)
                for obj_id, mask in enumerate(masks):
                    label_map[mask > 0.5] = obj_id + 1
                
                labels.append(label_map)
                
                # Create foreground/background mask
                fg_mask = label_map > 0
                fgbg.append(fg_mask.astype(np.uint8) * 255)
                
                # Create composite visualization
                comp = frames[frame_idx].copy()
                comp[~fg_mask] = comp[~fg_mask] * 0.3  # Dim background
                comps.append(comp)
            
            # Save segmentation files
            labels_folder = self.video_folder / 'labels'
            fgbg_folder = self.video_folder / 'fgbg'
            comps_folder = self.video_folder / 'comps'
            
            for folder in [labels_folder, fgbg_folder, comps_folder]:
                folder.mkdir(exist_ok=True)
            
            for idx, (label, fg, comp) in enumerate(zip(labels, fgbg, comps)):
                np.save(str(labels_folder / f'{idx+1:03d}.npy'), label)
                cv2.imwrite(str(fgbg_folder / f'{idx+1:03d}.png'), fg)
                cv2.imwrite(str(comps_folder / f'{idx+1:03d}.png'), comp)
            
            return {
                'labels': np.array(labels),
                'fgbg': np.array(fgbg),
                'comps': np.array(comps),
                'quality_score': segmentation_results.get('quality_score', 0.9)
            }
            
        except Exception as e:
            print(f"âŒ SAM2 segmentation error: {e}")
            return self._fallback_segmentation(frames)
    
    def _run_flowseek(self, frames: np.ndarray) -> Dict[str, Any]:
        """Run FlowSeek optical flow on frames"""
        try:
            flow_results = {
                'forward': [],
                'backward': [],
                'motion_parameters': []
            }
            
            for i in range(len(frames) - 1):
                # Compute forward flow
                # Use available method on FlowSeek engine
                if hasattr(self.flowseek_engine, 'compute_flow'):
                    forward_flow = self.flowseek_engine.compute_flow(frames[i], frames[i+1])  # type: ignore
                elif hasattr(self.flowseek_engine, 'forward'):
                    forward_flow = self.flowseek_engine.forward(frames[i], frames[i+1])  # type: ignore
                else:
                    forward_flow = np.zeros((frames[i].shape[0], frames[i].shape[1], 2))
                flow_results['forward'].append(forward_flow)
                
                # Compute backward flow
                # Use available method on FlowSeek engine  
                if hasattr(self.flowseek_engine, 'compute_flow'):
                    backward_flow = self.flowseek_engine.compute_flow(frames[i+1], frames[i])  # type: ignore
                elif hasattr(self.flowseek_engine, 'forward'):
                    backward_flow = self.flowseek_engine.forward(frames[i+1], frames[i])  # type: ignore
                else:
                    backward_flow = np.zeros((frames[i].shape[0], frames[i].shape[1], 2))
                flow_results['backward'].append(backward_flow)
                
                # Save flow files
                np.save(str(self.flow_folder / 'forward' / f'{i+1:03d}.npy'), forward_flow)
                np.save(str(self.flow_folder / 'backward' / f'{i+1:03d}.npy'), backward_flow)
                
                # Visualize flow
                if isinstance(forward_flow, np.ndarray) and forward_flow.ndim == 3:
                    flow_viz = self._visualize_flow(forward_flow)
                    cv2.imwrite(str(self.flow_folder / 'viz' / f'{i+1:03d}.png'), flow_viz)
            
            return flow_results
            
        except Exception as e:
            print(f"âŒ FlowSeek error: {e}")
            return self._fallback_optical_flow(frames)
    
    def _run_cotracker3(self, frames: np.ndarray, segmentation: Dict) -> Dict[str, Any]:
        """Run CoTracker3 point tracking"""
        try:
            # Convert frames to tensor
            frames_tensor = torch.from_numpy(frames).permute(0, 3, 1, 2).float() / 255.0
            frames_tensor = frames_tensor.to(self.device)
            
            # Track points
            # Use available method on CoTracker3 engine
            if hasattr(self.cotracker3_engine, 'track_video'):
                tracking_results = self.cotracker3_engine.track_video(frames_tensor, segmentation_masks=segmentation.get('labels') if segmentation else None)  # type: ignore
            elif hasattr(self.cotracker3_engine, 'track'):
                tracking_results = self.cotracker3_engine.track(frames_tensor)  # type: ignore
            else:
                tracking_results = {'tracks': [], 'confidence': 0.0}
            
            return tracking_results
            
        except Exception as e:
            print(f"âŒ CoTracker3 error: {e}")
            return self._fallback_tracking(frames, segmentation)
    
    def _fallback_segmentation(self, frames: np.ndarray) -> Dict[str, Any]:
        """Fallback segmentation using traditional methods"""
        print("ðŸ”„ Using traditional segmentation (Canny edge detection)")
        
        labels = []
        fgbg = []
        comps = []
        
        for frame in frames:
            # Simple edge-based segmentation
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            edges = cv2.Canny(gray, 50, 150)
            
            # Create simple labels using connected components
            _, label_map = cv2.connectedComponents(edges)
            labels.append(label_map)
            
            # Create foreground mask
            fg_mask = edges > 0
            fgbg.append(fg_mask.astype(np.uint8) * 255)
            
            # Create composite
            comp = frame.copy()
            comp[~fg_mask] = comp[~fg_mask] * 0.3
            comps.append(comp)
        
        return {
            'labels': np.array(labels),
            'fgbg': np.array(fgbg),
            'comps': np.array(comps),
            'quality_score': 0.5  # Low quality score for fallback
        }
    
    def _fallback_optical_flow(self, frames: np.ndarray) -> Dict[str, Any]:
        """Fallback optical flow using OpenCV"""
        print("ðŸ”„ Using traditional optical flow (Farneback)")
        
        flow_results = {
            'forward': [],
            'backward': [],
            'motion_parameters': []
        }
        
        for i in range(len(frames) - 1):
            gray1 = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
            gray2 = cv2.cvtColor(frames[i+1], cv2.COLOR_BGR2GRAY)
            
            # Compute forward flow using Farneback (without pre-allocated flow)
            forward_flow = cv2.calcOpticalFlowFarneback(
                gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0  # type: ignore
            )
            flow_results['forward'].append(forward_flow)
            
            # Compute backward flow  
            backward_flow = cv2.calcOpticalFlowFarneback(
                gray2, gray1, None, 0.5, 3, 15, 3, 5, 1.2, 0  # type: ignore
            )
            flow_results['backward'].append(backward_flow)
            
            # Save flow files
            self.flow_folder.mkdir(parents=True, exist_ok=True)
            (self.flow_folder / 'forward').mkdir(exist_ok=True)
            (self.flow_folder / 'backward').mkdir(exist_ok=True)
            (self.flow_folder / 'viz').mkdir(exist_ok=True)
            
            np.save(str(self.flow_folder / 'forward' / f'{i+1:03d}.npy'), forward_flow)
            np.save(str(self.flow_folder / 'backward' / f'{i+1:03d}.npy'), backward_flow)
            
            # Visualize flow
            flow_viz = self._visualize_flow(forward_flow)
            cv2.imwrite(str(self.flow_folder / 'viz' / f'{i+1:03d}.png'), flow_viz)
        
        return flow_results
    
    def _fallback_tracking(self, frames: np.ndarray, segmentation: Dict) -> Dict[str, Any]:
        """Fallback tracking using feature matching"""
        print("ðŸ”„ Using traditional tracking (feature matching)")
        
        tracking_results = {
            'tracks': [],
            'visibilities': [],
            'quality_score': 0.5
        }
        
        # Simple feature tracking using ORB
        try:
            orb = cv2.ORB_create()  # type: ignore
        except AttributeError:
            try:
                # Fallback to SIFT if ORB not available
                orb = cv2.SIFT_create()  # type: ignore
            except AttributeError:
                orb = None
        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
        
        prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
        if orb is not None:
            prev_kp, prev_desc = orb.detectAndCompute(prev_gray, None)
        else:
            prev_kp, prev_desc = [], None
        
        tracks = [[kp.pt for kp in prev_kp]]
        
        for frame in frames[1:]:
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            if orb is not None:
                kp, desc = orb.detectAndCompute(gray, None)
            else:
                kp, desc = [], None
            
            if prev_desc is not None and desc is not None:
                matches = matcher.match(prev_desc, desc)
                matches = sorted(matches, key=lambda x: x.distance)
                
                current_tracks = []
                for match in matches[:100]:  # Keep top 100 matches
                    current_tracks.append(kp[match.trainIdx].pt)
                
                tracks.append(current_tracks)
            else:
                tracks.append([])
            
            prev_gray = gray
            prev_kp = kp
            prev_desc = desc
        
        tracking_results['tracks'] = tracks
        return tracking_results
    
    def _visualize_flow(self, flow: np.ndarray) -> np.ndarray:
        """Visualize optical flow as HSV image"""
        h, w = flow.shape[:2]
        fx, fy = flow[:,:,0], flow[:,:,1]
        
        # Calculate magnitude and angle
        mag, ang = cv2.cartToPolar(fx, fy)
        
        # Create HSV image
        hsv = np.zeros((h, w, 3), dtype=np.uint8)
        hsv[..., 0] = ang * 180 / np.pi / 2  # Hue represents direction
        hsv[..., 1] = 255  # Full saturation
        # Normalize magnitude for value channel
        mag_norm = np.zeros_like(mag, dtype=np.uint8)
        cv2.normalize(mag, mag_norm, 0, 255, cv2.NORM_MINMAX)
        hsv[..., 2] = mag_norm  # Value represents magnitude
        
        # Convert to BGR for saving
        bgr = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)
        return bgr
    
    def _extract_shapes(self, results: Dict) -> Dict[str, Any]:
        """Extract and optimize shapes from segmentation and tracking"""
        shapes = {
            'shape_bank': {},
            'time_bank': {},
            'total_shapes': 0
        }
        
        if results['segmentation'] is not None:
            labels = results['segmentation']['labels']
            
            # Extract unique shapes across all frames
            shape_id = 0
            for frame_idx, label_map in enumerate(labels):
                unique_labels = np.unique(label_map)
                
                for label in unique_labels:
                    if label == 0:  # Skip background
                        continue
                    
                    # Extract shape mask
                    mask = (label_map == label)
                    
                    # Get shape properties
                    coords = np.argwhere(mask)
                    if len(coords) < self.config.min_cluster_size:
                        continue
                    
                    # Store shape
                    shapes['shape_bank'][shape_id] = {
                        'frame': frame_idx,
                        'mask': mask,
                        'coords': coords,
                        'centroid': coords.mean(axis=0),
                        'area': len(coords)
                    }
                    
                    # Store timing
                    if frame_idx not in shapes['time_bank']:
                        shapes['time_bank'][frame_idx] = []
                    shapes['time_bank'][frame_idx].append(shape_id)
                    
                    shape_id += 1
            
            shapes['total_shapes'] = shape_id
        
        return shapes
    
    def _save_outputs(self, results: Dict):
        """Save all outputs to disk"""
        # Save shape banks
        if results['shapes']:
            with open(self.output_folder / 'shape_bank.pkl', 'wb') as f:
                pickle.dump(results['shapes']['shape_bank'], f)
            
            with open(self.output_folder / 'time_bank.pkl', 'wb') as f:
                pickle.dump(results['shapes']['time_bank'], f)
        
        # Save motion data
        motion_data = {
            'video_name': self.video_name,
            'num_frames': len(results['frames']),
            'num_shapes': results['shapes']['total_shapes'] if results['shapes'] else 0,
            'quality_scores': {
                'segmentation': results['segmentation'].get('quality_score', 0) if results['segmentation'] else 0,
                'tracking': results['tracking'].get('quality_score', 0) if results['tracking'] else 0,
            },
            'active_engines': self.active_engines
        }
        
        with open(self.output_folder / 'motion_file.json', 'w') as f:
            json.dump(motion_data, f, indent=2)
        
        print(f"âœ… Saved shape_bank.pkl, time_bank.pkl, and motion_file.json")
        print(f"ðŸ“Š Total shapes extracted: {motion_data['num_shapes']}")
        print(f"ðŸ“Š Quality scores: Segmentation={motion_data['quality_scores']['segmentation']:.2f}, Tracking={motion_data['quality_scores']['tracking']:.2f}")


def main():
    """Main entry point for video processing"""
    parser = argparse.ArgumentParser(description='Process video with AI engines for motion vectorization')
    parser.add_argument('--video_file', type=str, required=True, help='Path to video file')
    parser.add_argument('--video_dir', type=str, default='videos', help='Directory containing videos')
    parser.add_argument('--output_dir', type=str, default='motion_vectorization/outputs', help='Output directory')
    parser.add_argument('--suffix', type=str, default=None, help='Output suffix')
    parser.add_argument('--max_frames', type=int, default=200, help='Maximum frames to process')
    parser.add_argument('--device', type=str, default='auto', choices=['auto', 'cuda', 'cpu'], help='Device to use')
    parser.add_argument('--no_sam2', action='store_true', help='Disable SAM2 engine')
    parser.add_argument('--no_cotracker3', action='store_true', help='Disable CoTracker3 engine')
    parser.add_argument('--no_flowseek', action='store_true', help='Disable FlowSeek engine')
    
    args = parser.parse_args()
    
    # Create configuration
    config = VideoProcessorConfig(
        video_file=args.video_file,
        video_dir=args.video_dir,
        output_dir=args.output_dir,
        suffix=args.suffix,
        max_frames=args.max_frames,
        device=args.device,
        use_sam2=not args.no_sam2,
        use_cotracker3=not args.no_cotracker3,
        use_flowseek=not args.no_flowseek
    )
    
    # Create processor and run
    processor = MotionVectorizationProcessor(config)
    results = processor.process_video()
    
    return results


if __name__ == '__main__':
    main()