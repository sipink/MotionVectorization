"""
Essential Motion Vectorization Utilities
Cleaned version containing only modern AI pipeline utilities.

This module provides core utility functions for:
- Frame/image processing and manipulation
- Shape analysis and geometric operations
- Mathematical transformations and decomposition
- Robust fallbacks for optional dependencies
- Input preprocessing and data handling

Legacy functions removed: deprecated Stanford 2023 system utilities,
unused optimization functions, legacy visualization code.
"""

import collections
import os
import re
import numpy as np
import cv2
import torch
from scipy.spatial.distance import cdist
import time
from scipy import ndimage
import torch.nn.functional as F

# Optional heavyweight dependencies with robust fallbacks
try:
    from skimage.segmentation import expand_labels

    SKIMAGE_AVAILABLE = True
except ImportError:
    SKIMAGE_AVAILABLE = False

    def expand_labels(label_image, distance=1, spacing=1):
        """Fallback implementation using cv2 morphological operations"""
        if distance <= 0:
            return label_image

        # Convert to uint8 for cv2 operations
        labels_uint8 = label_image.astype(np.uint8)
        kernel = cv2.getStructuringElement(
            cv2.MORPH_ELLIPSE, (2 * distance + 1, 2 * distance + 1)
        )

        # Dilate each label separately to avoid conflicts
        expanded = np.zeros_like(label_image)
        for label_id in np.unique(label_image):
            if label_id == 0:  # Skip background
                continue
            mask = (label_image == label_id).astype(np.uint8)
            dilated = cv2.dilate(mask, kernel, iterations=1)
            expanded[dilated > 0] = label_id

        return expanded


try:
    from kornia.filters import gaussian_blur2d

    KORNIA_AVAILABLE = True
except ImportError:
    KORNIA_AVAILABLE = False

    def gaussian_blur2d(
        input, kernel_size, sigma, border_type="reflect", separable=True
    ):
        """Fallback implementation using torch conv2d with gaussian kernel"""
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size, kernel_size)
        if isinstance(sigma, (int, float)):
            sigma = (sigma, sigma)

        # Create 2D gaussian kernel
        kx, ky = kernel_size
        sx, sy = sigma

        x = torch.arange(
            -(kx // 2), kx // 2 + 1, dtype=input.dtype, device=input.device
        )
        y = torch.arange(
            -(ky // 2), ky // 2 + 1, dtype=input.dtype, device=input.device
        )

        gauss_x = torch.exp(-0.5 * (x / sx) ** 2)
        gauss_y = torch.exp(-0.5 * (y / sy) ** 2)

        kernel = torch.outer(gauss_y, gauss_x)
        kernel = kernel / kernel.sum()

        # Reshape for conv2d: [out_channels, in_channels, height, width]
        kernel = kernel.expand(input.size(1), 1, kernel.size(0), kernel.size(1))

        # Apply convolution with padding
        padding = (kx // 2, ky // 2)
        return F.conv2d(input, kernel, padding=padding, groups=input.size(1))


try:
    from torchvision.transforms import Resize  # type: ignore[assignment]

    TORCHVISION_AVAILABLE = True
except ImportError:
    TORCHVISION_AVAILABLE = False

    class Resize:
        """Fallback implementation using torch interpolation"""

        def __init__(self, size, interpolation="bilinear"):
            if isinstance(size, int):
                self.size = (size, size)
            else:
                self.size = size
            self.interpolation = interpolation

        def __call__(self, tensor):
            if tensor.dim() == 3:
                tensor = tensor.unsqueeze(0)
                squeeze = True
            else:
                squeeze = False

            result = F.interpolate(
                tensor, size=self.size, mode=self.interpolation, align_corners=False
            )

            if squeeze:
                result = result.squeeze(0)

            return result


try:
    from pymatting import estimate_alpha_cf

    PYMATTING_AVAILABLE = True
except ImportError:
    PYMATTING_AVAILABLE = False

    def estimate_alpha_cf(
        image, trimap, preconditioner=None, laplacian_kwargs=None, cg_kwargs=None
    ):
        """Fallback implementation using simple trimap-based alpha estimation"""
        alpha = np.zeros(trimap.shape[:2], dtype=np.float64)

        # Known foreground (trimap == 1)
        alpha[trimap == 1.0] = 1.0

        # Known background (trimap == 0)
        alpha[trimap == 0.0] = 0.0

        # Unknown regions (trimap == 0.5) - use simple averaging
        unknown_mask = trimap == 0.5
        if np.any(unknown_mask):
            # Simple approach: set alpha based on image brightness
            if len(image.shape) == 3:
                gray = np.mean(image, axis=2)
            else:
                gray = image

            # Normalize to [0, 1] and use as alpha for unknown regions
            alpha[unknown_mask] = np.clip(gray[unknown_mask], 0.0, 1.0)

        return alpha


try:
    from pyefd import elliptic_fourier_descriptors

    PYEFD_AVAILABLE = True
except ImportError:
    PYEFD_AVAILABLE = False

    def elliptic_fourier_descriptors(
        contour, order=10, normalize=False, return_transformation=False
    ):
        """Fallback stub - pyefd not available"""
        # Return a stub implementation to prevent import errors
        if return_transformation:
            return np.zeros((order, 4)), (0, 0, 0)
        return np.zeros((order, 4))


# Import modern modules for compositing and sampling
from . import compositing
from . import sampling


# ============================================================================
# Core Mathematical Utilities
# ============================================================================


def decompose(A):
    """Decompose a 3x3 affine matrix into translation x rotation x shear x scale.

    Based on: https://caff.de/posts/4X4-matrix-decomposition/decomposition.pdf

    Args:
        A: 3x3 affine transformation matrix

    Returns:
        tuple: (tx, ty, sx, sy, theta, kx) transformation parameters
    """
    tx, ty = A[0, 2], A[1, 2]
    C = A[:2, :2]
    C_ = C.T @ C
    det_C = np.linalg.det(C)
    d_xx = np.sqrt(C_[0, 0])
    d_xy = C_[0, 1] / d_xx
    d_yy = np.sqrt(C_[1, 1] - d_xy**2)
    if det_C <= 0:
        d_xx = -d_xx
    D = np.array([[d_xx, d_xy], [0, d_yy]])
    R = C @ np.linalg.inv(D)
    theta = np.arctan2(R[1, 0], R[0, 0])
    sx = d_xx
    sy = d_yy
    kx = d_xy / sy
    return tx, ty, sx, sy, theta, kx


def sigmoid(x):
    """Standard sigmoid activation function."""
    return 1 / (1 + np.exp(-x))


def rotmax(rad):
    """Create 2D rotation matrix from angle in radians."""
    return np.array([[np.cos(rad), -np.sin(rad)], [np.sin(rad), np.cos(rad)]])


def params_to_mat(sx, sy, theta, kx, ky, tx=None, ty=None, k_first=False):
    """Convert transformation parameters to 3x3 affine matrix.

    Args:
        sx, sy: Scale factors
        theta: Rotation angle in radians
        kx, ky: Shear parameters
        tx, ty: Translation (optional, defaults to 0)
        k_first: Whether to apply shear before rotation

    Returns:
        3x3 affine transformation matrix
    """
    K = np.array([[1, kx], [ky, 1 + kx * ky]])
    R = rotmax(theta)
    S = np.array([[sx, 0], [0, sy]])
    if tx is None:
        tx = 0.0
    if ty is None:
        ty = 0.0
    if k_first:
        A_ = K @ R @ S
    else:
        A_ = R @ K @ S
    A = np.zeros((3, 3))
    A[:2, :2] = A_
    A[0, 2] = tx
    A[1, 2] = ty
    A[2, 2] = 1.0
    return A


# ============================================================================
# File and Directory Utilities
# ============================================================================


def get_numbers(dir):
    """Extract numerical indices from filenames in a directory.

    Used for processing video frame sequences.

    Args:
        dir: Directory path containing numbered files

    Returns:
        Sorted list of unique numerical indices
    """
    files = [os.path.splitext(f.name)[0].split("_")[0] for f in os.scandir(dir)]
    numbers = []
    for n in files:
        numbers_str = re.findall(r"\d+", n)
        numbers.extend([int(n_str) for n_str in numbers_str])
    return sorted(np.unique(numbers))


def save_frames(frames, path, suffix=None):
    """Save a sequence of frames to disk.

    Args:
        frames: List of frame arrays
        path: Output directory path
        suffix: Optional suffix for filenames
    """
    for i, frame in enumerate(frames):
        if suffix is not None:
            filename = f"{i + 1:03d}_{suffix}.png"
        else:
            filename = f"{i + 1:03d}.png"
        cv2.imwrite(os.path.join(path, filename), frame)


# ============================================================================
# Shape Analysis and Geometric Operations
# ============================================================================


def get_shape_coords(mask, thresh=0.0):
    """Get bounding box coordinates of a shape mask.

    Args:
        mask: Binary mask array
        thresh: Threshold for mask values

    Returns:
        tuple: (min_x, min_y, max_x, max_y) bounding box coordinates
    """
    if len(np.where(mask > thresh)[0]) < 1:
        return 0, 0, 0, 0
    min_x = np.min(np.where(mask > thresh)[1])
    max_x = np.max(np.where(mask > thresh)[1]) + 1
    min_y = np.min(np.where(mask > thresh)[0])
    max_y = np.max(np.where(mask > thresh)[0]) + 1
    return min_x, min_y, max_x, max_y


def get_shape_mask(labels, idx, expand=False, dtype=np.uint8):
    """Extract binary mask for a specific label.

    Args:
        labels: Label array
        idx: Label index to extract
        expand: Whether to add channel dimension
        dtype: Output data type

    Returns:
        Binary mask for the specified label
    """
    if expand:
        return np.array(dtype(labels == idx))[..., None]
    else:
        return np.array(dtype(labels == idx))


def get_shape_centroid(mask):
    """Calculate centroid of a shape mask.

    Args:
        mask: Binary mask array

    Returns:
        list: [centroid_x, centroid_y] coordinates
    """
    min_x, min_y, max_x, max_y = get_shape_coords(mask)
    return [(min_x + max_x - 1) / 2, (min_y + max_y - 1) / 2]


def is_valid_cluster(labels, l, min_cluster_size=25, min_dim=5, min_density=0.15):
    """Validate cluster based on size, dimensions, and density criteria.

    Args:
        labels: Label array
        l: Label to validate
        min_cluster_size: Minimum number of pixels
        min_dim: Minimum dimension size
        min_density: Minimum pixel density in bounding box

    Returns:
        bool: True if cluster meets all criteria
    """
    cluster_size = np.sum(labels == l)
    if cluster_size < min_cluster_size:
        return False
    min_x, min_y, max_x, max_y = get_shape_coords(np.uint8(labels == l))
    if max_x - min_x < min_dim:
        return False
    if max_y - min_y < min_dim:
        return False
    density = cluster_size / ((max_x - min_x) * (max_y - min_y))
    if density < min_density:
        return False
    else:
        return True


def get_alpha(mask, img, kernel_radius=5, bg_color=None, exclude=None, expand=False):
    """Extract alpha channel using trimap-based matting.

    Args:
        mask: Binary mask
        img: Input image
        kernel_radius: Kernel size for morphological operations
        bg_color: Background color (unused)
        exclude: Areas to exclude from processing
        expand: Whether to add channel dimension

    Returns:
        Alpha channel array
    """
    kernel = np.ones((kernel_radius, kernel_radius), np.uint8)
    mask_erode = cv2.erode(mask, kernel, iterations=1)
    if np.sum(mask_erode) < 1:
        mask_erode = mask.copy()
    trimap = cv2.dilate(mask, kernel, iterations=1)
    trimap[trimap != mask_erode] = 0.5
    if exclude is not None:
        trimap[exclude > 0] = 0
    alpha = estimate_alpha_cf(img / 255.0, trimap)
    if expand:
        return alpha[..., None]
    else:
        return alpha


def clean_labels(labels, spatial, min_cluster_size):
    """Clean label array by removing invalid clusters.

    Args:
        labels: Input label array
        spatial: Spatial information (unused)
        min_cluster_size: Minimum cluster size to keep

    Returns:
        Cleaned label array
    """
    labels_new = np.zeros_like(labels)
    for l in np.unique(labels):
        l_mask = np.array(labels == l, dtype=np.uint8)
        _, l_labels = cv2.connectedComponents(
            np.ascontiguousarray(l_mask, dtype=np.uint8)
        )
        for l_ in np.unique(l_labels):
            if l_ == 0:
                continue
            if not is_valid_cluster(l_labels, l_, min_cluster_size=min_cluster_size):
                l_mask = np.where(l_labels == l_, 0, l_mask).astype(np.uint8)
        labels_new[l_mask == 1] = l
    return labels_new


def compute_clusters_floodfill(
    fg_bg, edges, max_radius=3, min_cluster_size=50, min_density=0.15, min_dim=5
):
    """
    Compute clusters using connected components analysis.

    Note: This function replaces primitive trapped_ball_fill technology
    with modern connected components. For production use, SAM2.1 segmentation
    provides superior accuracy.

    Args:
        fg_bg: Foreground/background mask
        edges: Edge map
        max_radius: Maximum radius for expansion (unused)
        min_cluster_size: Minimum cluster size
        min_density: Minimum density threshold
        min_dim: Minimum dimension threshold

    Returns:
        tuple: (fillmap, fillmap_vis) cluster labels and visualization
    """
    # Modern alternative using connected components
    result = 255 * fg_bg * (1 - edges)
    result = np.uint8(result)

    # Use modern connected components analysis
    result_gray = (
        cv2.cvtColor(np.ascontiguousarray(result, dtype=np.uint8), cv2.COLOR_BGR2GRAY)
        if len(result.shape) == 3
        else result
    )
    num_labels, labels = cv2.connectedComponents(
        np.ascontiguousarray(result_gray, dtype=np.uint8)
    )
    fillmap = labels.astype(np.int32)

    # Remove invalid clusters using modern approach
    max_l_size = 0
    for l in np.unique(fillmap):
        if not is_valid_cluster(
            fillmap,
            l,
            min_cluster_size=min_cluster_size,
            min_density=min_density,
            min_dim=min_dim,
        ):
            fillmap[fillmap == l] = 0
        l_size = np.sum(np.uint8(fillmap == l))
        if l_size > max_l_size:
            max_l_size = l_size
    fillmap = expand_labels(fillmap, distance=2)

    # Modern visualization
    fillmap_vis = (
        np.uint8((fillmap / np.max(fillmap)) * 255)
        if np.max(fillmap) > 0
        else np.zeros_like(fillmap, dtype=np.uint8)
    )

    # Compress order of labels
    idx = -1
    for l in np.unique(fillmap):
        fillmap[fillmap == l] = idx
        idx += 1

    return fillmap, fillmap_vis


# ============================================================================
# Flow and Warping Operations
# ============================================================================


def warp_flo(x, flo, to_numpy=True):
    """
    Warp an image/tensor using optical flow.

    Args:
        x: Input tensor [B, C, H, W] (image to warp)
        flo: Flow tensor [B, 2, H, W] (optical flow)
        to_numpy: Whether to convert output to numpy

    Returns:
        Warped image/tensor
    """
    B, C, H, W = x.size()
    # Create mesh grid
    xx = torch.arange(0, W).view(1, -1).repeat(H, 1)
    yy = torch.arange(0, H).view(-1, 1).repeat(1, W)
    xx = xx.view(1, 1, H, W).repeat(B, 1, 1, 1)
    yy = yy.view(1, 1, H, W).repeat(B, 1, 1, 1)
    grid = torch.cat((xx, yy), 1).float()

    if x.is_cuda:
        grid = grid.cuda()
    vgrid = grid + flo
    # Scale grid to [-1,1]
    vgrid[:, 0, :, :] = 2.0 * vgrid[:, 0, :, :].clone() / max(W - 1, 1) - 1.0
    vgrid[:, 1, :, :] = 2.0 * vgrid[:, 1, :, :].clone() / max(H - 1, 1) - 1.0

    vgrid = vgrid.permute(0, 2, 3, 1)
    output = F.grid_sample(x, vgrid, mode="nearest")

    if to_numpy:
        output = output[0].permute(1, 2, 0)
        output = output.detach().cpu().numpy()
    return output


# ============================================================================
# Input Processing and Padding Utilities
# ============================================================================


class InputPadder:
    """Pads images such that dimensions are divisible by 8."""

    def __init__(self, dims, mode="sintel"):
        """
        Initialize padder.

        Args:
            dims: Input dimensions
            mode: Padding mode ('sintel' or other)
        """
        self.ht, self.wd = dims[-2:]
        pad_ht = (((self.ht // 8) + 1) * 8 - self.ht) % 8
        pad_wd = (((self.wd // 8) + 1) * 8 - self.wd) % 8
        if mode == "sintel":
            self._pad = [
                pad_wd // 2,
                pad_wd - pad_wd // 2,
                pad_ht // 2,
                pad_ht - pad_ht // 2,
            ]
        else:
            self._pad = [pad_wd // 2, pad_wd - pad_wd // 2, 0, pad_ht]

    def pad(self, *inputs):
        """Apply padding to input tensors."""
        return [F.pad(x, self._pad) for x in inputs]

    def unpad(self, x):
        """Remove padding from tensor."""
        ht, wd = x.shape[-2:]
        c = [self._pad[2], ht - self._pad[3], self._pad[0], wd - self._pad[1]]
        return x[..., c[0] : c[1], c[2] : c[3]]


# ============================================================================
# Compatibility and Utility Imports
# ============================================================================


# Provide access to colormap utility if available
def get_cmap(n):
    """Colormap generator utility"""
    import matplotlib.pyplot as plt

    try:
        return plt.colormaps["tab10"] if n <= 10 else plt.colormaps["viridis"]
    except (AttributeError, KeyError):
        return plt.cm.get_cmap("tab10") if n <= 10 else plt.cm.get_cmap("viridis")
